# =============================================================================
# Research Copilot Configuration (Python/Streamlit App)
# =============================================================================
# Copy this file to .env and update the values as needed.
# DO NOT commit .env to version control!
# =============================================================================

# -----------------------------------------------------------------------------
# Ollama Configuration (Local LLM)
# -----------------------------------------------------------------------------

# Ollama server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Default model to use (must be installed via `ollama pull <model>`)
# Recommended models: llama3.1:8b, mistral:7b, gemma3:4b
OLLAMA_DEFAULT_MODEL=llama3.1:8b

# -----------------------------------------------------------------------------
# MCP Server Configuration
# -----------------------------------------------------------------------------

# MCP server URL (default: http://localhost:3001)
MCP_SERVER_URL=http://localhost:3001

# MCP request timeout in seconds (default: 30)
MCP_TIMEOUT_SECONDS=30

# -----------------------------------------------------------------------------
# Application Settings
# -----------------------------------------------------------------------------

# Application environment: development, production (default: development)
APP_ENV=development

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------

# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
LOG_LEVEL=INFO

# Log format: pretty, json (default: pretty)
# - pretty: Human-readable colored output for development
# - json: Structured JSON for production/log aggregation
LOG_FORMAT=pretty
