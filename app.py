"""
Research Copilot - Streamlit Entry Point

A local AI research assistant that searches, reads, summarizes, and remembers.
All through transparent tool calls and zero cloud dependencies.
"""

import asyncio
import streamlit as st
from typing import Optional

from src.utils.config import settings
from src.utils.logger import setup_logger
from src.agent import Orchestrator, ResearchResponse
from src.models.research_mode import RESEARCH_MODE_OPTIONS, get_mode_key_from_label
from src.clients.ollama_client import OllamaClient
from src.clients.mcp_client import MCPClient
from src.ui.state import (
    init_state, get_state, add_message, add_tool_trace,
    set_researching, set_error, set_current_sources,
    clear_history, show_save_dialog, hide_save_dialog,
    get_tool_traces, set_selected_note, get_latest_tool_traces,
    _summarize_result
)
from src.ui.components import (
    render_error_message, render_save_note_dialog,
    render_confidence_meter,
    render_research_trail,
    render_followup_chips,  # Follow-ups now generated by orchestrator (DRY)
    get_error_recovery_steps, format_source_link, render_tags,
    render_chat_message, render_note_card,
    render_how_it_works
)

# Initialize logger
logger = setup_logger("research_copilot.app")


def get_event_loop():
    """Get or create an event loop."""
    try:
        return asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop


@st.cache_resource
def get_orchestrator() -> Orchestrator:
    """Get or create the orchestrator instance."""
    ollama = OllamaClient()
    mcp = MCPClient()
    return Orchestrator(
        ollama_client=ollama,
        mcp_client=mcp,
        model=settings.ollama_default_model,
        research_mode="quick"  # Default, updated per request
    )


def run_research(query: str, status_placeholder=None) -> Optional[ResearchResponse]:
    """Run a research query and return the response."""
    orchestrator = get_orchestrator()
    state = get_state()

    # Set research mode from UI state (PRD ¬ß4.5.2)
    orchestrator.set_research_mode(state.research_mode)
    orchestrator.set_fetch_extract_mode(state.fetch_extract_mode)
    orchestrator.set_model(state.model)

    set_researching(True)
    set_error(None)

    try:
        loop = get_event_loop()

        # Tool callbacks for UI updates per PRD ¬ß4.5.1
        def on_tool_start(tool_name: str, args: dict):
            logger.info(f"Tool starting: {tool_name}")
            # Update status placeholder with specific tool status
            if status_placeholder:
                if tool_name == "web_search":
                    status_placeholder.markdown("üîç **Searching web...**")
                elif tool_name == "fetch_page":
                    url = args.get("url", "")[:40]
                    status_placeholder.markdown(f"üìÑ **Reading sources...** `{url}...`")
                elif tool_name == "save_note":
                    status_placeholder.markdown("üíæ **Saving note...**")
                elif tool_name == "list_notes":
                    status_placeholder.markdown("üìù **Searching notes...**")
                elif tool_name == "get_note":
                    status_placeholder.markdown("üìù **Retrieving note...**")
                else:
                    status_placeholder.markdown(f"‚öôÔ∏è **Running {tool_name}...**")

        def on_tool_complete(tool_name: str, result: dict, success: bool):
            logger.info(f"Tool complete: {tool_name}, success={success}")
            # Update status back to generating
            if status_placeholder:
                status_placeholder.markdown("‚ú® **Generating response...**")

        # Run the research
        response = loop.run_until_complete(
            orchestrator.research(
                query,
                on_tool_start=on_tool_start,
                on_tool_complete=on_tool_complete
            )
        )

        # Update state with tool traces
        for trace in response.tool_trace:
            add_tool_trace(
                tool_name=trace.tool_name,
                arguments=trace.arguments,
                success=trace.success,
                duration_ms=trace.duration_ms,
                result=trace.result,
                error=trace.error,
                request_id=trace.request_id
            )

        # Update sources
        sources = [
            {"url": s.url, "title": s.title, "tool": s.tool}
            for s in response.sources
        ]
        set_current_sources(sources)

        return response

    except Exception as e:
        logger.error(f"Research failed: {e}")
        set_error(str(e))
        return None
    finally:
        set_researching(False)


async def check_services() -> dict:
    """Check if required services are available."""
    ollama = OllamaClient()
    mcp = MCPClient()

    ollama_ok = await ollama.is_available()
    mcp_status = await mcp.health()

    return {
        "ollama": ollama_ok,
        "mcp": mcp_status.available,
        "search_provider": mcp_status.search_provider
    }


def render_sidebar():
    """Render the sidebar with settings and notes."""
    state = get_state()

    st.markdown("## üîç Research Copilot")
    st.markdown("*Local AI Research Assistant*")
    st.divider()

    # Settings section
    st.markdown("### ‚öôÔ∏è Settings")

    # Model selection - only show preferred models that are installed
    loop = get_event_loop()
    ollama = OllamaClient()
    PREFERRED_MODELS = ["llama3.1:8b", "ministral-3:8b", "mistral:7b", "gemma3:4b"]
    DEFAULT_MODEL = "llama3.1:8b"

    model_options = [DEFAULT_MODEL]  # Fallback
    try:
        health = loop.run_until_complete(ollama.health())
        if health.available and health.models:
            # Only show preferred models that are installed
            installed_preferred = [m for m in PREFERRED_MODELS if m in health.models]
            model_options = installed_preferred if installed_preferred else [DEFAULT_MODEL]
    except Exception:
        pass

    # Default to llama3.1:8b if available
    default_model = DEFAULT_MODEL if DEFAULT_MODEL in model_options else model_options[0]

    selected_model = st.selectbox(
        "Model",
        options=model_options,
        index=model_options.index(state.model) if state.model in model_options else model_options.index(default_model) if default_model in model_options else 0,
        help="Select the Ollama model to use",
    )
    state.model = selected_model

    # Research mode - using centralized config (DRY)
    research_mode = st.selectbox(
        "Research Mode",
        options=RESEARCH_MODE_OPTIONS,
        index=0 if state.research_mode == "quick" else 1,
        help="Quick: 3 sources, bullet points. Deep: 5 sources, detailed analysis.",
    )
    state.research_mode = get_mode_key_from_label(research_mode)

    # Fetch output format
    extract_mode = st.selectbox(
        "Fetch Format",
        options=["text", "markdown"],
        index=0 if state.fetch_extract_mode == "text" else 1,
        help="Choose text or markdown formatting for fetched pages.",
    )
    state.fetch_extract_mode = extract_mode

    st.divider()

    # Notes section
    st.markdown("### üìù Saved Notes")
    notes_search = st.text_input(
        "üîç Search notes",
        placeholder="Search...",
        key="notes_search",
        label_visibility="collapsed"
    )
    if notes_search != state.notes_search_query:
        state.notes_search_query = notes_search
        state.notes_offset = 0

    # Fetch notes from MCP
    loop = get_event_loop()
    try:
        mcp = MCPClient()
        notes_result = loop.run_until_complete(
            mcp.list_notes(
                query=notes_search if notes_search else None,
                limit=state.notes_page_size,
                offset=state.notes_offset,
            )
        )

        if notes_result.success and notes_result.data:
            notes = notes_result.data.get("notes", [])
            total_count = notes_result.data.get("total_count", len(notes))
            has_more = notes_result.data.get("has_more", False)
            if notes:
                start = state.notes_offset + 1
                end = state.notes_offset + len(notes)
                st.caption(f"Showing {start}-{end} of {total_count}")
                def on_view(note_id: str) -> None:
                    set_selected_note(note_id)
                    st.rerun()

                for note in notes:
                    render_note_card(
                        note_id=note.get("id", ""),
                        title=note.get("title", "Untitled"),
                        tags=note.get("tags", []),
                        created_at=note.get("created_at", ""),
                        snippet=note.get("snippet", ""),
                        on_click=on_view,
                    )
                    st.divider()

                col_prev, col_next = st.columns(2)
                with col_prev:
                    if st.button(
                        "‚óÄ Prev",
                        disabled=state.notes_offset == 0,
                        key="notes_prev",
                    ):
                        state.notes_offset = max(0, state.notes_offset - state.notes_page_size)
                        st.rerun()
                with col_next:
                    if st.button(
                        "Next ‚ñ∂",
                        disabled=not has_more,
                        key="notes_next",
                    ):
                        state.notes_offset = state.notes_offset + state.notes_page_size
                        st.rerun()
            else:
                st.caption("_No notes found_")
        else:
            st.caption("_Notes from your research will appear here_")
    except Exception as e:
        logger.debug(f"Could not fetch notes: {e}")
        st.caption("_MCP server needed for notes_")

    # Clear history button
    if st.button("üóëÔ∏è Clear History", help="Clear chat history"):
        clear_history()
        st.rerun()

    st.divider()

    # Status section
    st.markdown("### üìä Status")

    # Check services (cached)
    loop = get_event_loop()
    services = loop.run_until_complete(check_services())

    col1, col2 = st.columns(2)
    with col1:
        if services["ollama"]:
            st.success("Ollama ‚úì")
        else:
            st.error("Ollama ‚úó")

    with col2:
        if services["mcp"]:
            st.success("MCP ‚úì")
        else:
            st.warning("MCP ‚úó")

    # Show search provider
    search_provider = services.get("search_provider")
    if search_provider:
        provider_display = "üîç Serper API" if search_provider == "serper" else "ü¶Ü DuckDuckGo"
        st.caption(f"Search: {provider_display}")

    if not services["ollama"]:
        st.caption("‚ö†Ô∏è Start Ollama to enable research")
    if not services["mcp"]:
        st.caption("‚ö†Ô∏è Start MCP server for tools")

    st.divider()

    # About section
    st.markdown("### ‚ÑπÔ∏è About")
    st.caption(f"Model: {settings.ollama_default_model}")
    st.caption(f"MCP: {settings.mcp_server_url}")
    st.caption(f"Environment: {settings.app_env}")


def render_note_viewer(note_id: str):
    """Render a note viewer for the selected note."""
    loop = get_event_loop()
    try:
        mcp = MCPClient()
        result = loop.run_until_complete(mcp.get_note(note_id))

        if result.success and result.data:
            note = result.data.get("note", {})

            st.markdown(f"## üìù {note.get('title', 'Untitled')}")

            # Tags
            tags = note.get("tags", [])
            if tags:
                st.markdown(render_tags(tags), unsafe_allow_html=True)

            st.divider()

            # Content
            st.markdown(note.get("content", ""))

            # Source URLs
            source_urls = note.get("source_urls", [])
            if source_urls:
                with st.expander("üìö Sources", expanded=False):
                    for i, url in enumerate(source_urls, 1):
                        st.markdown(format_source_link(i, url, url, max_title_len=50))

            # Metadata
            st.caption(f"Created: {note.get('created_at', '')[:10]}")

            # Close button
            if st.button("‚Üê Back to Chat"):
                set_selected_note(None)
                st.rerun()
        else:
            st.error("Could not load note")
            if st.button("‚Üê Back"):
                set_selected_note(None)
                st.rerun()
    except Exception as e:
        st.error(f"Error loading note: {e}")
        if st.button("‚Üê Back"):
            set_selected_note(None)
            st.rerun()


def render_main_content():
    """Render the main chat interface."""
    state = get_state()

    # If a note is selected, show the note viewer instead
    if state.selected_note_id:
        render_note_viewer(state.selected_note_id)
        return

    # Handle pending follow-up questions (PRD ¬ß5.10)
    pending_followup = st.session_state.pop("pending_followup", None)

    st.title("Research Copilot")
    st.markdown("Ask about any topic and I'll search, read, and summarize for you.")

    # Research Trail panel (PRD ¬ß5.10 Demo Polish Pack)
    # Show tool traces for the last 3 queries
    query_groups = get_latest_tool_traces(max_queries=3)
    if query_groups:
        render_research_trail(
            query_groups=query_groups,
            expanded=False,
            mcp_endpoint=settings.mcp_server_url
        )

    # Chat container
    chat_container = st.container()

    # Display existing messages
    with chat_container:
        for i, msg in enumerate(st.session_state.get("messages", [])):
            role = msg.get("role", "user")
            content = msg.get("content", "")
            sources = msg.get("sources", [])
            msg_title = msg.get("suggested_title", "Research Notes")
            can_save = msg.get("can_save_as_note", True)
            show_save = role == "assistant" and content and can_save and not content.startswith("üöß")

            saved = render_chat_message(
                role=role,
                content=content,
                sources=sources,
                show_sources=True if role == "assistant" else False,
                sources_title=f"üìö {len(sources)} Sources" if sources else "üìö Sources",
                show_save_button=show_save,
                save_button_key=f"save_{i}",
            )

            if saved:
                show_save_dialog(
                    title=msg_title,
                    content=content,
                    source_urls=[s.get("url", "") for s in sources]
                )

    # Render follow-up chips for the LAST assistant message (PRD ¬ß5.10)
    messages = st.session_state.get("messages", [])
    if messages:
        last_msg = messages[-1]
        if last_msg.get("role") == "assistant":
            followups = last_msg.get("followup_questions", [])
            if followups:
                selected_followup = render_followup_chips(
                    followups, key_prefix=f"followup_last"
                )
                if selected_followup:
                    st.session_state["pending_followup"] = selected_followup
                    st.rerun()

    # Error display with recovery steps (PRD ¬ß6)
    if state.error:
        # Determine error code from error message
        error_code = "unknown"
        if "ollama" in state.error.lower():
            error_code = "ollama_unavailable"
        elif "mcp" in state.error.lower():
            error_code = "mcp_server_unavailable"
        elif "timeout" in state.error.lower():
            error_code = "fetch_timeout"
        elif "search" in state.error.lower():
            error_code = "search_failed"

        render_error_message(
            message=state.error,
            suggestion="Check if Ollama and MCP server are running.",
            error_code=error_code,
            recovery_steps=get_error_recovery_steps(error_code),
            title="Research Error"
        )

    # Save note dialog
    if state.show_save_dialog:
        prefill = state.save_note_prefill
        note_data = render_save_note_dialog(
            prefill_title=prefill.get("title", ""),
            prefill_content=prefill.get("content", ""),
            prefill_tags=prefill.get("tags", []),
            source_urls=prefill.get("source_urls", [])
        )

        if note_data:
            # Save note via MCP client
            try:
                loop = get_event_loop()
                mcp = MCPClient()
                save_result = loop.run_until_complete(
                    mcp.save_note(
                        title=note_data["title"],
                        content=note_data["content"],
                        tags=note_data["tags"],
                        source_urls=note_data.get("source_urls", [])
                    )
                )
                if save_result.success:
                    st.success(f"Note '{note_data['title']}' saved!")
                else:
                    st.error(f"Failed to save note: {save_result.error}")
            except Exception as e:
                st.error(f"Could not save note: {e}")
            hide_save_dialog()
            st.rerun()

        if st.button("Cancel"):
            hide_save_dialog()
            st.rerun()

    # Chat input (also handles pending follow-up clicks)
    chat_input = st.chat_input("Ask about a topic or paste a URL...")
    prompt = pending_followup or chat_input

    if prompt:
        # Add user message to history
        add_message("user", prompt)

        # Display user message
        with chat_container:
            with st.chat_message("user"):
                st.markdown(prompt)

        # Check if services are available
        loop = get_event_loop()
        services = loop.run_until_complete(check_services())

        if not services["ollama"]:
            with chat_container:
                with st.chat_message("assistant"):
                    st.error("‚ùå Ollama is not available. Please start Ollama first.")
            add_message("assistant", "‚ùå Ollama is not available. Please start Ollama first.")
        elif not services["mcp"]:
            with chat_container:
                with st.chat_message("assistant"):
                    st.warning(
                        "‚ö†Ô∏è MCP server is not running. Research tools are unavailable.\n\n"
                        "Start the MCP server with: `cd mcp_server && npm start`"
                    )
            add_message(
                "assistant",
                "‚ö†Ô∏è MCP server is not running. Research tools are unavailable."
            )
        else:
            # Run research
            with chat_container:
                # Use status placeholder for specific tool status (PRD ¬ß4.5.1)
                status_placeholder = st.empty()
                status_placeholder.markdown("üîç **Starting research...**")
                response = run_research(prompt, status_placeholder)
                status_placeholder.empty()  # Clear status when done

                if response:
                    if response.sources:
                        sources = [
                            {"url": s.url, "title": s.title}
                            for s in response.sources
                        ]
                    else:
                        sources = []

                    # Show save button immediately for new response
                    msg_index = len(st.session_state.get("messages", []))
                    saved = render_chat_message(
                        role="assistant",
                        content=response.content,
                        sources=sources,
                        show_sources=True,
                        sources_title=f"üìö {len(sources)} Sources" if sources else "üìö Sources",
                        show_save_button=response.can_save_as_note,
                        save_button_key=f"save_new_{msg_index}",
                    )
                    if saved:
                        show_save_dialog(
                            title=response.suggested_title,
                            content=response.content,
                            source_urls=[s["url"] for s in sources] if sources else []
                        )

                    # Show confidence meter
                    render_confidence_meter(len(response.sources))

                    # Build tool traces for this message
                    msg_tool_traces = [
                        {
                            "tool_name": t.tool_name,
                            "arguments": t.arguments,
                            "success": t.success,
                            "duration_ms": t.duration_ms,
                            "result_summary": _summarize_result(t.tool_name, t.result) if t.result else t.error or "",
                            "request_id": t.request_id or response.request_id
                        }
                        for t in response.tool_trace
                    ]

                    # Add to message history with sources, metadata, tool traces, and follow-ups (PRD ¬ß5.10)
                    followups = response.followup_questions if response.followup_questions else []
                    add_message(
                        "assistant",
                        response.content,
                        sources if sources else None,
                        can_save_as_note=response.can_save_as_note,
                        suggested_title=response.suggested_title,
                        followup_questions=followups,
                        tool_traces=msg_tool_traces,
                        query=prompt
                    )

                    # Rerun to update Research Trail panel at top of page
                    st.rerun()
                else:
                    st.error("Research failed. Please try again.")
                    add_message("assistant", "‚ùå Research failed. Please try again.")
                    st.rerun()


def main() -> None:
    """Main entry point for the Streamlit app."""
    # Page configuration
    st.set_page_config(
        page_title="Research Copilot",
        page_icon="üîç",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    # Custom CSS - Design tokens from PRD ¬ß7 & CLAUDE.md
    st.markdown("""
        <style>
        /* Base styling per PRD Design Tokens */
        .stApp {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            font-size: 16px;
            font-weight: 400;
            color: #1A1A1A;
        }

        /* Chat messages - per PRD styling */
        .stChatMessage {
            padding: 12px 16px;
            border-radius: 8px;
            margin-bottom: 12px;
        }

        /* User messages */
        [data-testid="stChatMessageContent"]:has(.stMarkdown) {
            background: #FFFFFF;
            border: 1px solid #E9ECEF;
        }

        /* Assistant messages */
        .stChatMessage[data-testid="assistant"] {
            background: #F8F9FA;
        }

        /* Sidebar styling */
        [data-testid="stSidebar"] {
            background-color: #F8F9FA;
        }

        [data-testid="stSidebar"] h2 {
            color: #1A1A1A;
            font-size: 1.25rem;
        }

        /* Expanders */
        .stExpander {
            border: 1px solid #E9ECEF;
            border-radius: 8px;
            margin: 8px 0;
        }

        /* Source cards */
        .source-card {
            border: 1px solid #E9ECEF;
            border-radius: 8px;
            padding: 12px;
            margin: 8px 0;
            background: #F8F9FA;
        }

        /* Tag pills */
        .tag-pill {
            background: #E7F1FF;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 12px;
            margin-right: 4px;
            color: #0066CC;
        }

        /* Tool trace styling */
        .tool-trace {
            font-family: 'SF Mono', Monaco, 'Courier New', monospace;
            font-size: 13px;
            background: #F8F9FA;
            padding: 8px 12px;
            border-radius: 4px;
            margin: 4px 0;
        }

        /* Success/Error indicators */
        .status-success {
            color: #198754;
        }

        .status-error {
            color: #DC3545;
        }

        /* Progress bar styling */
        .stProgress > div > div {
            background-color: #0066CC;
        }

        /* Button styling */
        .stButton > button {
            border-radius: 6px;
            font-weight: 500;
        }

        /* Note card in sidebar */
        .note-card {
            background: #FFFFFF;
            border: 1px solid #E9ECEF;
            border-radius: 8px;
            padding: 8px 12px;
            margin: 4px 0;
        }

        /* Code blocks */
        code {
            background: #E9ECEF;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 13px;
        }

        /* Links */
        a {
            color: #0066CC;
        }

        /* Dividers */
        hr {
            border-color: #E9ECEF;
            margin: 16px 0;
        }

        /* Small text per PRD Typography */
        .stCaption, small, .metadata {
            font-size: 14px;
            color: #6C757D;
        }

        /* Source pills per PRD Design Tokens */
        .source-pill {
            background: #E7F1FF;
            padding: 4px 12px;
            border-radius: 16px;
            font-size: 13px;
            color: #0066CC;
        }

        /* Spacing per PRD - tight (4px, 8px), normal (12px, 16px), loose (24px, 32px) */
        .tight-spacing { padding: 4px 8px; }
        .normal-spacing { padding: 12px 16px; }
        .loose-spacing { padding: 24px 32px; }

        /* Citation superscript styling per PRD ¬ß5.10 */
        sup.citation a {
            color: #0066CC;
            text-decoration: none;
            font-size: 0.75em;
        }

        /* Confidence meter colors */
        .confidence-high { color: #198754; }
        .confidence-medium { color: #FFC107; }
        .confidence-low { color: #DC3545; }
        </style>
    """, unsafe_allow_html=True)

    # Initialize session state
    init_state()

    # Sidebar
    with st.sidebar:
        render_sidebar()

    # Main content with tabs
    tab_research, tab_about = st.tabs(["üîç Research", "‚ÑπÔ∏è How It Works"])

    with tab_research:
        render_main_content()

    with tab_about:
        render_how_it_works()


if __name__ == "__main__":
    logger.info("Starting Research Copilot")
    main()
