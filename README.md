# Research Copilot

A local AI research assistant that searches, reads, summarizes, and remembersâ€”all through transparent tool calls and zero cloud dependencies.

## Features

- **Web Search & Fetch**: Search the web (Serper/DuckDuckGo) and read page content with source attribution
- **Research Notes**: Save, tag, and search your research findings locally
- **Transparent Tool Calls**: See exactly what the AI is doing (Research Trail with last 3 queries)
- **Source Citations**: Inline numbered citations with clickable links
- **Local-First**: Runs entirely on your machine using Ollama
- **MCP Protocol**: Standardized tool access via Model Context Protocol (JSON-RPC 2.0)
- **LLM-Generated Follow-ups**: Contextual follow-up question suggestions after each response
- **Multi-Query Research Trail**: View MCP tool calls grouped by query with timing and success metrics
- **How It Works Tab**: In-app documentation explaining the architecture and key concepts

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Streamlit UI (app.py)                     â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Sidebar   â”‚  â”‚           Research Trail                  â”‚ â”‚
â”‚  â”‚             â”‚  â”‚           - Tool executions               â”‚ â”‚
â”‚  â”‚  - Settings â”‚  â”‚           - Source badges                 â”‚ â”‚
â”‚  â”‚  - Notes    â”‚  â”‚           - Timing info                   â”‚ â”‚
â”‚  â”‚  - Modes    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚  - Status   â”‚  â”‚           Chat Panel                      â”‚ â”‚
â”‚  â”‚             â”‚  â”‚           - Messages                      â”‚ â”‚
â”‚  â”‚             â”‚  â”‚           - Citations                     â”‚ â”‚
â”‚  â”‚             â”‚  â”‚           - Follow-up suggestions         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Agent Orchestrator                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Prompts   â”‚  â”‚   Parser    â”‚  â”‚     Orchestration       â”‚  â”‚
â”‚  â”‚   Builder   â”‚  â”‚  Tool Calls â”‚  â”‚        Loop             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                       â”‚
          â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama Client     â”‚              â”‚       MCP Client            â”‚
â”‚   (Local LLM)       â”‚              â”‚   (JSON-RPC 2.0)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚      MCP Server (Node.js)   â”‚
                                     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                     â”‚  â”‚web_searchâ”‚ â”‚fetch_page â”‚  â”‚
                                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                     â”‚  â”‚save_noteâ”‚ â”‚list_notes â”‚  â”‚
                                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
                                     â”‚  â”‚get_note â”‚  SQLite+FTS5   â”‚
                                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Prerequisites

- **Python 3.11+**
- **Node.js 18+**
- **Ollama** with a model installed (e.g., `llama3.1:8b`)

## Quick Start

### 1. Clone and Install Python Dependencies

```bash
cd research-copilot
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Install and Start MCP Server

```bash
cd mcp_server
npm install
npm run build
npm start  # Runs on port 3001
```

### 3. Start Ollama

```bash
# In a separate terminal
ollama serve
ollama pull llama3.1:8b  # Or your preferred model
```

### 4. Run the Application

```bash
# In the project root, with .venv activated
streamlit run app.py
```

Open http://localhost:8501 in your browser.

## Project Structure

```
research-copilot/
â”œâ”€â”€ app.py                    # Streamlit entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/                # LLM orchestration
â”‚   â”‚   â”œâ”€â”€ orchestrator.py   # Tool-calling loop
â”‚   â”‚   â”œâ”€â”€ parser.py         # Tool call extraction
â”‚   â”‚   â”œâ”€â”€ prompts.py        # System prompt builder
â”‚   â”‚   â””â”€â”€ citations.py      # Citation formatting
â”‚   â”œâ”€â”€ clients/              # External service clients
â”‚   â”‚   â”œâ”€â”€ ollama_client.py  # Ollama API client
â”‚   â”‚   â””â”€â”€ mcp_client.py     # MCP JSON-RPC client
â”‚   â”œâ”€â”€ models/               # Data models
â”‚   â”‚   â”œâ”€â”€ research_mode.py  # Research mode config (single source of truth)
â”‚   â”‚   â”œâ”€â”€ notes.py          # Note models
â”‚   â”‚   â””â”€â”€ responses.py      # API response models
â”‚   â”œâ”€â”€ ui/                   # Streamlit components
â”‚   â”‚   â”œâ”€â”€ components.py     # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ design_tokens.py  # Centralized design values
â”‚   â”‚   â””â”€â”€ state.py          # Session state management
â”‚   â”œâ”€â”€ api/                  # API layer
â”‚   â”‚   â”œâ”€â”€ research.py       # Research operations
â”‚   â”‚   â””â”€â”€ notes.py          # Notes operations
â”‚   â”œâ”€â”€ errors/               # Error handling
â”‚   â””â”€â”€ utils/                # Utilities
â”‚       â”œâ”€â”€ config.py         # Pydantic settings
â”‚       â”œâ”€â”€ logger.py         # Structured logging
â”‚       â””â”€â”€ validators.py     # URL validation
â”œâ”€â”€ mcp_server/               # MCP Server (Node.js/TypeScript)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ tools/            # Tool implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ webSearch.ts  # DuckDuckGo/Serper search
â”‚   â”‚   â”‚   â”œâ”€â”€ fetchPage.ts  # Page content extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ saveNote.ts   # Note persistence
â”‚   â”‚   â”‚   â”œâ”€â”€ listNotes.ts  # Note listing/search
â”‚   â”‚   â”‚   â””â”€â”€ getNote.ts    # Note retrieval
â”‚   â”‚   â”œâ”€â”€ db/               # SQLite + FTS5
â”‚   â”‚   â”œâ”€â”€ middleware/       # Rate limiting
â”‚   â”‚   â””â”€â”€ server.ts         # Express + JSON-RPC
â”‚   â””â”€â”€ tests/                # Vitest tests
â”œâ”€â”€ tests/                    # Python tests (pytest)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ validate_env.py       # Environment validation
â””â”€â”€ docs/                     # Documentation
    â”œâ”€â”€ HOW_IT_WORKS.md       # Architecture overview
    â”œâ”€â”€ SETUP.md              # Installation guide
    â”œâ”€â”€ API.md                # Python API reference
    â”œâ”€â”€ MCP_SERVER.md         # MCP tools reference
    â”œâ”€â”€ PERFORMANCE.md        # Performance analysis
    â””â”€â”€ SECURITY.md           # Security audit
```

## Configuration

Copy the example environment files and customize as needed:

```bash
cp .env.example .env
cp mcp_server/.env.example mcp_server/.env
```

### Root `.env` (Streamlit App)

```env
# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3.1:8b

# MCP Server
MCP_SERVER_URL=http://localhost:3001

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=pretty  # or "json"
```

### MCP Server `.env` (Required for Serper API)

Create a `.env` file in `mcp_server/`:

```env
# Search Provider (duckduckgo or serper)
SEARCH_PROVIDER=serper

# Serper API Key (required if SEARCH_PROVIDER=serper)
SERPER_API_KEY=your_key_here
```

**Note:** The MCP server reads its own `.env` file for search configuration. DuckDuckGo is the default (no API key required) but may have rate limits. Serper provides more reliable results. See `mcp_server/.env.example` for all available options including rate limiting and timeouts.

## Research Modes

| Mode | Sources | Fetch | Description |
|------|---------|-------|-------------|
| **Quick Summary** | 5 max | 3 pages | Concise bullet points, < 250 words |
| **Deep Dive** | 7 max | 5 pages | Detailed analysis with recommendations |

**Notes:**
- Notes list pagination uses `limit` + `offset` in MCP, and the UI shows snippets for each note
- Model selection shows 4 preferred models: `llama3.1:8b` (default), `ministral-3:8b`, `mistral:7b`, `gemma3:4b`
- Research Trail displays tool calls for the last 3 queries with per-query and aggregate statistics
- Sidebar shows active search provider (ğŸ” Serper API or ğŸ¦† DuckDuckGo)

## MCP Tools

| Tool | Purpose | Parameters |
|------|---------|------------|
| `web_search` | Search the web | `query`, `limit` (1-5), `provider` |
| `fetch_page` | Read page content | `url`, `max_chars`, `extract_mode` |
| `save_note` | Save research | `title`, `content`, `tags`, `source_urls` |
| `list_notes` | Find notes | `query`, `tags`, `limit`, `offset` |
| `get_note` | Retrieve note | `id` (UUID) |

## Research Trail

The Research Trail panel shows MCP tool calls for transparency and debugging:

```
ğŸ”¬ Research Trail (12 MCP calls from 3 queries)
â”œâ”€â”€ ğŸ–¥ï¸ MCP Server Connection
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â”‚ 3       â”‚ 12        â”‚ 83%         â”‚ 3521ms     â”‚
â”‚   â”‚ Queries â”‚ Calls     â”‚ Success     â”‚ Total Time â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   MCP Endpoint: http://localhost:3001/mcp
â”‚   Protocol: JSON-RPC 2.0
â”‚
â”œâ”€â”€ ğŸ”µ Latest
â”‚   Query: "What is quantum computing?"
â”‚   ğŸ“Š 4 calls | âœ… 3 success | â±ï¸ 1842ms
â”‚   â”œâ”€â”€ âœ… ğŸ” web_search `MCP TOOL`     [712ms]
â”‚   â””â”€â”€ âœ… ğŸ“„ fetch_page `MCP TOOL`     [523ms]
â”‚
â”œâ”€â”€ âšª Query 2
â”‚   Query: "How does machine learning work?"
â”‚   ...
```

**Features:**
- Shows last 3 queries with their tool traces (configurable)
- Per-query statistics (calls, success rate, timing)
- Expandable request details showing JSON-RPC 2.0 payloads
- Success/failure indicators with timing for each tool call

## Development

### Run Tests

```bash
# Python tests
pytest tests/ -v

# MCP Server tests
cd mcp_server && npm test
```

### Validate Environment

```bash
python scripts/validate_env.py
```

### Code Style

- Python: Black, isort, type hints
- TypeScript: ESLint, Prettier

## Key Design Principles

1. **Sources Over Assertions**: Every answer cites its sources
2. **Transparency Is Trust**: Users see all tool executions
3. **Local-First Always**: No cloud LLM dependencies
4. **Research Persists**: Notes survive app restarts
5. **Errors Guide Recovery**: Helpful error messages with suggestions

## Documentation

- **[How It Works](docs/HOW_IT_WORKS.md)**: Architecture overview and key concepts
- **[Setup Guide](docs/SETUP.md)**: Detailed installation and troubleshooting
- **[API Reference](docs/API.md)**: Python API documentation
- **[MCP Server](docs/MCP_SERVER.md)**: MCP tools and configuration
- **[Performance](docs/PERFORMANCE.md)**: Performance analysis and optimization
- **[Security](docs/SECURITY.md)**: Security audit report

## License

MIT

---

*Built as a demonstration of AI agent orchestration patterns with MCP protocol.*
